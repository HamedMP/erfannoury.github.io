---
layout: post
category: Deep Learning, Machine Learning
title: دنیای یادگیری عمیق در سال 2015
published: private
---
به نام خدا
===========

سال 2015 باز شاهد گسترش خیلی زیاد یادگیری عمیق توی تقریباً همه‌ی حوزه‌ها بودیم. سعی می‌کنم توی این پست هر آنچه که یادم مونده از اتفاقاتی که در این حوزه افتاده رو بنویسم. کاملاً موارد biased هستن و احتمالاً هیچ ترتیب خاصی نداشته باشن.

خب چون خیلی از اتفاقات ابتدای سال یادم نیست اول سعی می‌کنم اتفاقات این ماه آخر رو بنویسم.

ماه دسامبر سال 2015 در دنیای یادگیری عمیق چند تا اتفاق خیلی مهم افتادن.

اولین کنفرانس NIPS 2015 بود که باز هم تعداد مقالات در مورد یادگیری عمیق زیاد شده بودن و البته بیشتر هم در این مورد صحبت می‌شد. یکی توییت کرده بود که همه‌ی اعضای Google Brain و Google DeepMind اومده بودن برای تعطیلات مونترال! و واقعاً هم حضور زیادی داشتن. البته از نظر تعداد مقالات آزمایشگاه ژوبین قهرمانی جزو اولین‌ها بود. بالاخره اینا از خیلی وقت پیش روی کارهای تئوری زیاد کار می‌کردن و اخیراً هم وارد یادگیری عمیق شدن. تا جایی که من اطلاع دارم در حال حاضر کارهای Yarin Gal از این آزمایشگاه بیشتر مطرحه.  
اولین کاری که ازش دیده بودم با 
[این پست](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html) بود. بحث در مورد این بود که چطوری میشه فهمید یک مدل عمیق چه چیزهایی رو چقدر یاد گرفته. به طور خلاصه روشی که پیشنهاد داده بود این بود که در مرحله‌ی آزمون هم DropOut رو فعال نگه داریم، کلی خروجی از شبکه بگیریم (با یه ورودی ثابت) و بعد از روی این خروجی‌ها در مورد قطعیت شبکه در مورد خروجی مربوط به اون ورودی قضاوت کنیم. توی 
[این مقاله](http://arxiv.org/abs/1506.02142) در این مورد نوشته. توصیه می‌کنم اول پستش رو بخونید بعد مقاله رو اگر خیلی بیشتر مشتاق بودید.
پست دوم Yarin Gal که باز در مورد استفاده از DropOut در approximate Bayesian inference هست، این بار روی شبکه‌های RNN تمرکز کرده. کلیت صحبتش (توی 
[این پست](http://mlg.eng.cam.ac.uk/yarin/blog_5058.html) و 
[این مقاله](http://arxiv.org/abs/1512.05287) اینه که روش‌های پیشین استفاده از DropOut باعث میشن که شبکه نتونه الگو در زمان رو یاد بگیره. بدون این regularization هم شبکه خیلی زود overfit می‌کنه. پس راه‌حل پیشنهادی اینه که یک ماسک DropOut رو برای همه‌ی گام‌های زمانی (time steps) استفاده کنیم. یعنی اینکه یه بار از یه توزیع برنولی متغیرهای تصادفی رو نمونه‌گیری کنیم و از این متغیرهای تصادفی برای همه‌ی گام‌های زمانی استفاده کنیم. به نظر میرسه که این کارش هنوز خیلی کامل نیست و توی وبلاگش هم داره در موردش بحث میشه. به نظر کار جالبی میاد و تکنیک خوبی می‌تونه باشه. نکته‌ی مثبت دیگه‌ی مقاله‌اش اینه که آخر مقاله یه نمونه کد از پیاده‌سازی این کار رو توی Theano با استفاده از کتابخونه‌ی keras آورده.

یکی از اتفاقات خیلی خبرسازی که در طول NIPS 2015 افتاد اعلام تاسیس شرکت جدید *غیرانتفاعی* 
[OpenAI](https://openai.com/blog/introducing-openai/) 
 بود که توسط ایلان ماسک، موسس شرکت‌های SpaceX و Tesla، و Sam Altman از Y Combinator تغذیه‌ی مالی خواهد شد. جالبه که یکی دیگه از سرمایه‌گذاری‌های این شرکت Peter Thiel هست که یکی از سرمایه‌گذارهای شرکت Palantir هم هست، شرکتی که الان ارزشی بیشتر از 20 میلیارد دلار داره و تقریباً هیچ اطلاع عمومی از کارهایی که داره انجام میده وجود نداره، ولی گویا مشتری‌های این شرکت دولت‌ها، مثل دولت امریکا هستن.
 طبق پستی که به عنوان معرفی این شرکت گذاشته شده نوشته شده که این شرکت با هدف تحقیق در زمینه‌ی هوش مصنوعی «دوستانه» (Friendly AI) ایجاد شده و هدفش اینه که جلوی اتفاقات ناگوار در اثر پیشرفت هوش مصنوعی رو بگیره. با وجود اینکه موسسان شرکت قول دادن که از نظر انتشار نتایج تحقیقات و همچنین ثبت‌اختراع بسیار باز عمل خواهند کرد، ولی مشخصه که در دسترس بودن الگوریتم‌ها و روش‌ها به معنی در دسترس بودن کار تحقیقاتی نیست. در این وسط عوامل مختلف دیگه‌ای هم هستن، مثل داشتن داده‌های مناسب و البته توان پردازشی کافی، که باعث میشه استفاده از اون تحقیق برای همه میسر نباشه. درسته که اعلامیه‌ی ماموریت این شرکت در نگاه اول خیلی دلگرم‌کننده به نظر میرسه، ولی بعید می‌دونم سرنوشت متفاوتی از شرکت‌های فعلی فعال در این زمینه (مثل گوگل، گوگل دیپ‌مایند، فیسبوک، بایدو و ...) داشته باشه. درسته که همه‌ی اینها خیلی زود و به صورت باز نتایج تحقیقات خودشون رو منتشر می‌کنن، ولی قطعاً کل کار تحقیقاتی در اون نتایج و روش‌ها خلاصه نمیشه و فاکتورهای دیگه‌ای هم وجود داره که در دسترس بقیه نیست. جدای از قول تامین بودجه‌ی بسیار زیاد (یک میلیارد دلار) برای این شرکت، نکته‌ی خیلی مهم در رابطه با OpenAI اینه که تونسته افراد شاخصی مثل Ilya Sutskever و Andrej Karpathy رو استخدام کنه. این در حالیه که Ilya Sutskever توی گوگل کار می‌کرد و Andrej هم امسال کارآموزی‌اش رو در DeepMind گذروند.
 خلاصه با وجود اینکه از همون اول خبر تاسیس این شرکت خیلی سر و صدا کرد، ولی افراد مهمی از جنبه‌های دیگه‌ای به این خبر نگاه کردن و نظراتشون رو گفتن. فکر می‌کنم یکی از بهترین این نوشته‌ها مربوط به Neil Lawrence باشه که توی [مجله‌ی گاردین](http://www.theguardian.com/media-network/2015/dec/14/openai-benefit-humanity-data-sharing-elon-musk-peter-thiel) منتشر شده.
 
 کلاً امسال هم NIPS مقالات زیادی در مورد یادگیری عمیق به خودش دید. با توجه به اینکه NIPS خیلی به کارهای با پشتوانه‌ی تئوری بالا علاقه داره ولی باز هم مقالات متعددی در این زمینه قبول شده بودن. در کنار کنفرانس اصلی هم کلی کارگاه و برنامه‌ی جانبی در مورد یادگیری عمیق برگزار شد. مثلاً یکی‌شون یه برنامه‌ی آموزشی در مورد یادگیری عمیق بود که قرار بود توسط Yoshua Bengio، Yann LeCun و Geoff Hinton برگزار بشه، ولی Hinton توی جلسه حضور نداشت. هنوز خودم کامل نگاه نکردم ولی بسیار ازشمنده ([لینک ویدیو](http://research.microsoft.com/apps/video/default.aspx?id=259574&l=i)).
 
 علاوه بر اون ویدیوی ارائه‌ی مقاله‌ی "Deep Visual Analogy-Making" هم خیلی جالب بود به نظرم
 ([لینک ویدیو](http://research.microsoft.com/apps/video/default.aspx?id=259628&l=i)).
 
 اونقدر اتفاق تو این NIPS 2015 در مورد یادگیری عمیق زیاد بود که فکر می‌کنم لازمه یه پست جدا در موردش بنویسم.   
 
 
 قضیه‌ی ثبت‌اختراعات
 
 از روی مقالاتی که امسال چاپ شدن، به نظر میرسه که کار روی مدل‌هایی که attention دارن و همچنین ترکیب یادگیری عمیق با Reinforcement Learning خیلی بیشتر شده بود. فکر می‌کنم DeepMind و برکلی سهم زیادی در مطرح کردن بیش از پیش ترکیب یادگیری عمیق و Reinforcement Learning دارن.
 ترند‌های امسال (مثل attention و reinforcement learning)
 
 hypergrad
 
 
 امسال شاهد خیل عظیم کتابخانه‌های یادگیری عمیق بودیم. کلی کتابخونه‌ی خیلی خوب معرفی شدن که کلی هم (بعضی‌هاشون) سر و صدا به پا کردن.

یکی از این کتابخونه‌ها که اولای امسال معرفی شد و خیلی هم زود افراد زیادی رو به خودش جذب کرد کتابخانه‌ی Keras بود. این کتابخونه در ابتدا بر مبنای کتابخونه‌ی سطح پایین Theano بود، ولی در سطح بالا یک محیط برنامه‌نویسی شبیه Torch7 رو ارائه می‌کرد. بخاطر همین تقریباً همه‌ی سختی‌های کار با Theano رو تا حد زیادی برطرف می‌کرد. همین شد که خیلی زود کلی طرفدار پیدا کرد و الان هم یکی از کتابخانه‌های بسیار مطرح در زمینه‌ی یادگیری عمیقه. نویسنده‌ی این کتابخونه هم الان کارمند گوگله.

چند ماه بعد هم کتابخونه‌ی MXNet معرفی شد. این کتابخونه حاصل مشارکت چند نفر هست که هر کدوم یه سری کتابخونه‌ی یادگیری ماشین خیلی خوب داشتن، مثلاً یکی از افراد این پروژه‌ همونی هست که کتابخونه‌ی بسیار عالی xgboost رو نوشته. این کتابخونه با C++ نوشته شده، بخاطر همین خیلی راحت میشه ازش توی زبان‌های دیگه‌ای استفاده کرد. در حال حاضر wrapper این کتابخونه برای پایتون، جولیا و جاوا اسکریپت وجود داره (تا جایی که من اطلاع دارم). به نظر میاد که کتابخونه‌ی خوبی باشه، بخصوص که قابلیت محاسبات توزیع‌یافته بر روی بیش از یک کارت گرافیکی رو هم داره. حتماً تو برنامه‌ام هست که این کتابخونه رو بررسی کنم.

چند ماه بعد از معرفی این کتابخونه هم خبرش در اومد که توسعه‌ی کتابخونه‌ی PyLearn2 دیگه متوقف شده. این کتابخونه هم محصول آزمایشگاه LISA دانشگاه مونترال هست (کسایی که Theano رو توسعه دادن). میشه گفت جزو اولین کتابخونه‌های یادگیری عمیق بود، ولی دیگه کسی ازش استفاده نمی‌کرد و الانم دیگه کنار گذاشته شده.
 
 Keras, CNTK, TensorFlow, MXNet, PyLearn2
 
 
 Deep Dream, StyleNet
 
 ILSVRC 2015