---
layout: post
category: Deep Learning, Tutorial
title: کلاس یادگیری عمیق آکسفورد، جلسه‌ی دوم
published: private
---
به نام خدا
===========

ترم بهار 2015، Nando de Freitas از دانشگاه آکسفورد، کلاس درس یادگیری عمیق رو ارائه داد. تصمیم گرفتم که من هم از مطالب این کلاس درس استفاده کنم و چیزهایی در این مورد یاد بگیرم. 
با توجه به اینکه عادت دارم در هنگام خوندن و یاد گرفتن چیزی حتماً یادداشت بردارم، تصمیم گرفتم که برای هر درس از این کلاس، یادداشت‌هام رو کمی مرتب کنم و به صورت منظم اینجا در قالب پست بنویسم. هم اینطوری بهتر مستند میشه، هم بیشتر فرصت هست در موردش فکر کنم و هم اینکه شاید به درد کس دیگه‌ای هم بخوره. 
بنابراین یادداشت‌های مربوط به اولین جلسه رو توی این پست می‌نویسم.


شروع
=====

قبل از شروع بهتره بگم که سایت اصلی درس در [این آدرس](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/) قرار داره.
 می‌تونید برید و از اونجا فایل‌های مربوط به درس رو بردارید. البته فیلم‌های کلاس روی یوتوب قرار داره. سعی می‌کنم در طول زمان این فایل‌ها رو روی تخته‌سفید بذارم و لینک بدم تا بشه راحت فیلم‌ها رو هم دید.

همونطور که احتمالاً تو عنوان پست هم دیدید، شروع کار رو جلسه‌ی دوم گذاشتم. بخاطر اینکه جلسه‌ی اول «معرفی» نام داره و واقعاً در حد معرفی کلی یادگیری ماشین هست. توصیه می‌کنم خودتون اون رو ببینید. چیزی در مورد جلسه‌ی اول نمی‌نویسم اینجا. ولی از جلسه‌ی دوم شروع می‌کنم.


مدل‌های خطی
===========
عنوان جلسه‌ی دوم «مدل‌های خطی» هست که در اون به به صورت ساده‌ای، با استفاده از مدل‌های خطی به شروع کار پرداخته.
در یادگیری ماشین، (در واقع در یادگیری ماشین تحتِ نظارت یا Supervised Learning) تعدادی داده یا ورودی یا نمونه‌گیری به همراه اطلاعات دیگه‌ای در مورد اون داده‌ها داده شده و هدف اینه که برای یک داده‌ی جدید، اطلاع مورد نظر رو پیشبینی کنیم. 

برای مثال فرض کنید مجموعه داده و اطلاعی (که بهش مجموعه‌ی آموزش میگیم) داده شده. به صورت رسمی این مجموعه رو میشه اینطوری نوشت:

<div>$$ \{ \mathbf{x}_{1:n}, \mathbf{y}_{1:n} \} \equiv \{(\mathbf{x}_1,\mathbf{y}_1), (\mathbf{x}_2,\mathbf{y}_2), \dots, (\mathbf{x}_n,\mathbf{y}_n) \} $$</div>

یعنی مجموعه‌ای از جفت‌های ورودی و خروجی که در اینجا \\(\mathbf{x} _ i\\) ورودی‌ها هستن و \\(\mathbf{y} _ i\\) خروجی‌ها هستن.
حالا فرض کنید یک داده‌ی جدید داده شده، داده‌ای که قبلاً دیده نشده (مثل \\( \mathbf{x} _ {n+1} \\)). هدف اینه که مقدار اطلاع رو برای این داده‌ی جدید، یعنی \\( \mathbf{y} _ {n+1} \\)، رو پیشبینی کنیم.
برای اینکار لازمه تابعی رو پیدا کنیم که بتونه این مقدار رو برامون پیشبینی کنه.